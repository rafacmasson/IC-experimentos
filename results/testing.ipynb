{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import (\n",
        "    accuracy_score, balanced_accuracy_score, classification_report,\n",
        "    confusion_matrix, precision_score, recall_score, f1_score\n",
        ")\n",
        "from joblib import load"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "datasets = ['binary_cic_ids_2017', 'cic_ids_2017', 'binary_nsl_kdd', 'nsl_kdd', 'binary_unsw_nb15', 'unsw_nb15']\n",
        "\n",
        "dataset = datasets[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Selecting the classifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifiers = ['AB', 'KNN', 'LDA', 'LR', 'NB', 'RF']\n",
        "\n",
        "classifier = classifiers[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xxwfZxRWFfd"
      },
      "outputs": [],
      "source": [
        "# Load the trained model\n",
        "model = load(f'../trained-models/{dataset}_{classifier}.joblib')\n",
        "\n",
        "# Load the test data\n",
        "test = pd.read_csv(f\"../processed-datasets/{dataset}_test_normalized.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Splitting into X and y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_test = test.iloc[:, :-1]\n",
        "y_test = test.iloc[:, -1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start time\n",
        "start_time_test = time.time()\n",
        "\n",
        "# Make predictions on the test data\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# End time\n",
        "end_time_test = time.time()\n",
        "\n",
        "# Testing time\n",
        "testing_time = end_time_test - start_time_test\n",
        "\n",
        "print(f'Testing Time: {testing_time} seconds')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding the testing time to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to load the CSV file\n",
        "try:\n",
        "    times = pd.read_csv(f\"../results/{dataset}_testing_times.csv\")\n",
        "except:\n",
        "    # If the file does not exist, create an empty DataFrame\n",
        "    times = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_line = pd.DataFrame({'Classifier': [f'{classifier}'],\n",
        "                         'Testing Time': round(testing_time, 4)})\n",
        "\n",
        "times = pd.concat([times, new_line], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "times"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the DataFrame\n",
        "times.to_csv(f\"../results/{dataset}_testing_times.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Accuracy, Balanced Accuracy, and Accuracy per Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Accuracy\n",
        "total_accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {round(total_accuracy, 4)}')\n",
        "\n",
        "# Balanced Accuracy\n",
        "balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
        "print(f'Balanced Accuracy: {round(balanced_accuracy, 4)}')\n",
        "\n",
        "# Dictionary to store the accuracy of each class\n",
        "class_accuracies = {}\n",
        "\n",
        "class_labels = sorted(list(set(y_test)))\n",
        "\n",
        "# Iterate over all unique classes present in the true labels\n",
        "for class_label in class_labels:\n",
        "    # Filter predictions and true labels for the current class\n",
        "    true_class_indices = [i for i, label in enumerate(y_test) if label == class_label]\n",
        "    class_predictions = [y_pred[i] for i in true_class_indices]\n",
        "    true_class_labels = [y_test[i] for i in true_class_indices]\n",
        "    \n",
        "    # Calculate accuracy for the current class\n",
        "    class_accuracy = accuracy_score(true_class_labels, class_predictions)\n",
        "    \n",
        "    # Store the class accuracy in the dictionary\n",
        "    class_accuracies[class_label] = class_accuracy\n",
        "\n",
        "# Rounding the values in the dictionary\n",
        "rounded_dictionary = {key: round(value, 4) for key, value in class_accuracies.items()}\n",
        "\n",
        "# Print the accuracy of each class\n",
        "for class_label, accuracy in class_accuracies.items():\n",
        "    print(f'Accuracy of class {class_label}: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Precision, Recall, and F1-Score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Precision, Recall, and F1-score\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "print(f'Precision: {round(precision, 4)}')\n",
        "print(f'Recall: {round(recall, 4)}')\n",
        "print(f'F1-score: {round(f1, 4)}\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Metrics per class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Metrics per class\n",
        "report = classification_report(y_test, y_pred)\n",
        "print(f'Metrics per class:\\n{report}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Confusion Matrix\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\n",
        "plt.title('Confusion Matrix')\n",
        "plt.xlabel('Predicted Labels')\n",
        "plt.ylabel('True Labels')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Adding the metric values to a DataFrame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Try to load the CSV file\n",
        "try:\n",
        "    evaluation = pd.read_csv(f\"../results/{dataset}_models_evaluation.csv\")\n",
        "except:\n",
        "    # If the file does not exist, create an empty DataFrame\n",
        "    evaluation = pd.DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a row to the DataFrame with the provided data\n",
        "row_data = {\n",
        "    'Classifier': classifier,\n",
        "    'Accuracy': round(total_accuracy, 4),\n",
        "    'Balanced Accuracy': round(balanced_accuracy, 4),\n",
        "    'Precision': round(precision, 4),\n",
        "    'Recall': round(recall, 4),\n",
        "    'F1-Score': round(f1, 4)\n",
        "}\n",
        "\n",
        "row_data.update(class_accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_line = pd.DataFrame([row_data])\n",
        "\n",
        "evaluation = pd.concat([evaluation, new_line], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Saving the DataFrame\n",
        "evaluation.to_csv(f\"../results/{dataset}_models_evaluation.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "318f2c383d48220c821fefd227250c35bb828f95042f5889a5ad1704b1baf79c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
